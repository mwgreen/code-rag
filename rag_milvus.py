"""
RAG with Milvus Lite + MLX embeddings.

Each project gets its own DB file at {project_root}/.code-rag/milvus.db.
The HTTP server shares the MLX model across projects and caches Milvus clients per DB.
CLI and stdio MCP use ephemeral connections.
"""

from pymilvus import MilvusClient, DataType
from mlx_embeddings.utils import load as mlx_load, generate as mlx_generate
import mlx.core as mx
from contextlib import contextmanager
from pathlib import Path
from typing import List, Dict, Optional
import asyncio
import os
import hashlib
import chunking  # Clean chunking utilities (no PyTorch/ChromaDB)

# Embedding model configuration
# Qodo-Embed-1-1.5B: state-of-the-art code embedding, 1536 dims, Qwen2 architecture
# Prefers local Q8 quantized model (~1.6 GB) over full-precision HF model (~5.8 GB)
_SCRIPT_DIR = Path(__file__).parent
_LOCAL_Q8_MODEL = _SCRIPT_DIR / "models" / "qodo-embed-1-1.5b-mlx-q8"
_DEFAULT_MODEL = str(_LOCAL_Q8_MODEL) if _LOCAL_Q8_MODEL.exists() else "Qodo/Qodo-Embed-1-1.5B"
_MODEL_PATH = os.getenv("EMBED_MODEL_PATH", _DEFAULT_MODEL)
_EMBED_DIM = 1536

# Auto-enable offline mode if using HF model ID and it's cached
if not Path(_MODEL_PATH).exists():
    _model_cache = Path.home() / ".cache/huggingface/hub/models--Qodo--Qodo-Embed-1-1.5B"
    if _model_cache.exists() and 'HF_HUB_OFFLINE' not in os.environ:
        os.environ['HF_HUB_OFFLINE'] = '1'


def compute_file_hash(file_path: str) -> str:
    """Compute SHA256 hash of file content for change detection."""
    try:
        with open(file_path, 'rb') as f:
            return hashlib.sha256(f.read()).hexdigest()
    except Exception:
        return ""


def is_jaxb_generated(file_path: str) -> bool:
    """Check if a Java file is JAXB-generated by looking for the marker."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = [f.readline() for _ in range(50)]
            content = ''.join(lines)
            if 'schema fragment specifies the expected content' in content:
                return True
            if 'generated by the JavaTM Architecture for XML Binding' in content:
                return True
            if '@XmlRootElement' in content and '@XmlType' in content:
                return True
        return False
    except Exception:
        return False


# Configuration
COLLECTION_NAME = "codebase"

# Global MLX model (load once, reuse — this is the expensive resource)
_mlx_model = None
_mlx_tokenizer = None


def get_mlx_model():
    """Get or load MLX model (one-time load)."""
    global _mlx_model, _mlx_tokenizer

    if _mlx_model is not None:
        return _mlx_model, _mlx_tokenizer

    _is_local = Path(_MODEL_PATH).exists()
    _label = "local Q8" if _is_local else "HuggingFace"
    print(f"Loading Qodo-Embed-1-1.5B ({_label}) from {_MODEL_PATH}...")
    _mlx_model, _mlx_tokenizer = mlx_load(_MODEL_PATH)
    print(f"Qodo-Embed-1-1.5B ready ({_EMBED_DIM} dims, {_label})")

    return _mlx_model, _mlx_tokenizer


def embed_texts(texts: List[str]) -> List[List[float]]:
    """Embed texts using MLX."""
    model, tokenizer = get_mlx_model()
    output = mlx_generate(model, tokenizer, texts=texts)
    embeddings = output.text_embeds.tolist()
    mx.clear_cache()
    return embeddings


# --- Client management ---

# Ephemeral session client (CLI batch indexing)
_active_client = None

# Persistent client cache: db_path -> MilvusClient (HTTP server)
_persistent_clients: Dict[str, MilvusClient] = {}

# Async concurrency primitives (initialized once for HTTP server)
_write_lock: Optional[asyncio.Lock] = None
_embed_semaphore: Optional[asyncio.Semaphore] = None
_server_mode = False


def init_server_mode():
    """Initialize async concurrency primitives for HTTP server mode.
    Persistent clients are created lazily per db_path."""
    global _write_lock, _embed_semaphore, _server_mode
    _write_lock = asyncio.Lock()
    _embed_semaphore = asyncio.Semaphore(1)
    _server_mode = True
    print("Server mode initialized (persistent clients will be created per project)")


def close_server_mode():
    """Close all persistent clients and reset server mode."""
    global _write_lock, _embed_semaphore, _server_mode
    for path, client in _persistent_clients.items():
        client.close()
        print(f"Closed persistent client: {path}")
    _persistent_clients.clear()
    _write_lock = None
    _embed_semaphore = None
    _server_mode = False


def _get_persistent_client(db_path: str) -> MilvusClient:
    """Get or create a persistent client for the given DB path."""
    if db_path not in _persistent_clients:
        Path(db_path).parent.mkdir(parents=True, exist_ok=True)
        _persistent_clients[db_path] = MilvusClient(db_path)
        print(f"Opened persistent client: {db_path}")
    return _persistent_clients[db_path]


def _resolve_db_path(db_path: Optional[str]) -> str:
    """Resolve a db_path. Raises if not provided — every caller must supply one."""
    if not db_path:
        raise ValueError("db_path is required. Each project stores its index at {project}/.code-rag/milvus.db")
    return db_path


def _ensure_collection(client):
    """Create collection if it doesn't exist."""
    if not client.has_collection(COLLECTION_NAME):
        print(f"Creating Milvus collection: {COLLECTION_NAME} (dim={_EMBED_DIM})")
        client.create_collection(
            collection_name=COLLECTION_NAME,
            dimension=_EMBED_DIM,
            metric_type="COSINE",
        )
        print(f"Collection created: {COLLECTION_NAME}")


@contextmanager
def milvus_session(db_path: Optional[str] = None):
    """Open a long-lived session for batch operations (indexing).
    All milvus_client() calls within this context reuse the same connection.
    If server mode is active, reuses the persistent client."""
    global _active_client
    path = _resolve_db_path(db_path)

    if _server_mode:
        # HTTP server — reuse persistent client
        client = _get_persistent_client(path)
        _ensure_collection(client)
        old_active = _active_client
        _active_client = client
        try:
            yield client
        finally:
            _active_client = old_active
    else:
        # CLI / stdio — open fresh connection
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        client = MilvusClient(path)
        _ensure_collection(client)
        _active_client = client
        try:
            yield client
        finally:
            client.close()
            _active_client = None


@contextmanager
def milvus_client(db_path: Optional[str] = None):
    """Get a Milvus client.
    Priority: persistent client (server) > active session (batch) > fresh ephemeral."""
    global _active_client
    path = _resolve_db_path(db_path)

    if _server_mode:
        client = _get_persistent_client(path)
        _ensure_collection(client)
        yield client  # Don't close — server lifetime
    elif _active_client is not None:
        yield _active_client  # Reuse session — don't close
    else:
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        client = MilvusClient(path)
        _ensure_collection(client)
        try:
            yield client
        finally:
            client.close()


# --- Core operations ---

def file_needs_indexing(file_path: str, db_path: Optional[str] = None) -> bool:
    """Check if file needs (re)indexing by comparing content hash."""
    abs_path = str(Path(file_path).absolute())
    current_hash = compute_file_hash(file_path)
    if not current_hash:
        return True

    try:
        with milvus_client(db_path) as client:
            results = client.query(
                collection_name=COLLECTION_NAME,
                filter=f'path == "{abs_path}"',
                limit=1,
                output_fields=["content_hash"]
            )
            if results and len(results) > 0:
                stored_hash = results[0].get('content_hash', '')
                if stored_hash == current_hash:
                    return False
            return True
    except Exception:
        return True


def add_documents(documents: List[str], metadatas: List[Dict], ids: List[str],
                  content_hash: Optional[str] = None, db_path: Optional[str] = None) -> int:
    """Add documents to Milvus with MLX embeddings."""
    embeddings = embed_texts(documents)

    data = []
    for i, (doc, meta, doc_id, emb) in enumerate(zip(documents, metadatas, ids, embeddings)):
        int_id = hash(doc_id) & 0x7FFFFFFFFFFFFFFF

        doc_data = {
            "id": int_id,
            "vector": emb,
            "document": doc,
            "doc_id": doc_id,
            **meta
        }
        if content_hash:
            doc_data["content_hash"] = content_hash
        data.append(doc_data)

    with milvus_client(db_path) as client:
        client.insert(collection_name=COLLECTION_NAME, data=data)

    return len(documents)


def search(query: str, n: int = 5, type_filter: Optional[str] = None,
           language_filter: Optional[str] = None, db_path: Optional[str] = None) -> List[Dict]:
    """Search for similar documents."""
    query_embedding = embed_texts([query])[0]

    filters = []
    if type_filter:
        filters.append(f'type == "{type_filter}"')
    if language_filter:
        filters.append(f'language == "{language_filter}"')
    filter_expr = " && ".join(filters) if filters else None

    with milvus_client(db_path) as client:
        results = client.search(
            collection_name=COLLECTION_NAME,
            data=[query_embedding],
            limit=n,
            filter=filter_expr,
            output_fields=["document", "path", "language", "type", "doc_id"]
        )

    formatted = []
    if results and len(results) > 0:
        for hit in results[0]:
            formatted.append({
                'content': hit['entity']['document'],
                'path': hit['entity'].get('path', ''),
                'language': hit['entity'].get('language', ''),
                'type': hit['entity'].get('type', ''),
                'distance': hit['distance']
            })

    return formatted


def delete_by_path(file_path: str, db_path: Optional[str] = None) -> int:
    """Delete all chunks for a given file path."""
    abs_path = str(Path(file_path).absolute())
    try:
        with milvus_client(db_path) as client:
            results = client.query(
                collection_name=COLLECTION_NAME,
                filter=f'path == "{abs_path}"',
                output_fields=["id"]
            )
            if results:
                client.delete(
                    collection_name=COLLECTION_NAME,
                    filter=f'path == "{abs_path}"'
                )
            return len(results)
    except Exception:
        return 0


def clear_collection(db_path: Optional[str] = None):
    """Clear all data."""
    with milvus_client(db_path) as client:
        if client.has_collection(COLLECTION_NAME):
            client.drop_collection(COLLECTION_NAME)
            print(f"Collection cleared")


def _query_all(output_fields: list, batch_size: int = 1000, db_path: Optional[str] = None) -> list:
    """Query rows from Milvus with offset pagination. Capped at 16384 by Milvus Lite."""
    MILVUS_MAX = 16384
    all_results = []
    offset = 0

    with milvus_client(db_path) as client:
        if not client.has_collection(COLLECTION_NAME):
            return []
        while offset < MILVUS_MAX:
            effective_limit = min(batch_size, MILVUS_MAX - offset)
            batch = client.query(
                collection_name=COLLECTION_NAME,
                filter="",
                limit=effective_limit,
                offset=offset,
                output_fields=output_fields
            )
            if not batch:
                break
            all_results.extend(batch)
            if len(batch) < effective_limit:
                break
            offset += effective_limit

    return all_results


def get_stats(db_path: Optional[str] = None) -> Dict:
    """Get index statistics. Uses row_count for totals, offset query for breakdowns."""
    with milvus_client(db_path) as client:
        if not client.has_collection(COLLECTION_NAME):
            return {'total_chunks': 0, 'total_files': 0, 'by_language': {}, 'by_type': {}}
        stats = client.get_collection_stats(COLLECTION_NAME)
    total_docs = stats['row_count']

    results = _query_all(["path", "language", "type"], db_path=db_path)
    unique_files = set(r['path'] for r in results)
    sampled = len(results) < total_docs

    by_language = {}
    for r in results:
        lang = r.get('language', 'unknown')
        by_language[lang] = by_language.get(lang, 0) + 1

    by_type = {}
    for r in results:
        t = r.get('type', 'unknown')
        by_type[t] = by_type.get(t, 0) + 1

    result = {
        'total_chunks': total_docs,
        'total_files': len(unique_files),
        'by_language': by_language,
        'by_type': by_type
    }
    if sampled:
        result['note'] = f'Stats based on {len(results)}/{total_docs} chunks (Milvus offset limit). File count is approximate.'
    return result


def list_indexed_files(db_path: Optional[str] = None) -> Dict:
    """List all indexed files grouped by type."""
    results = _query_all(["path", "type"], db_path=db_path)

    by_type = {}
    for r in results:
        file_type = r.get('type', 'unknown')
        path = r.get('path', '')
        if file_type not in by_type:
            by_type[file_type] = set()
        by_type[file_type].add(path)

    return {t: sorted(list(files)) for t, files in by_type.items()}


def add_file(path: str, force: bool = False, db_path: Optional[str] = None) -> int:
    """Add a file to Milvus (uses chunking for chunking)."""
    abs_path = str(Path(path).absolute())

    if not force and not file_needs_indexing(abs_path, db_path):
        return 0

    try:
        with milvus_client(db_path) as client:
            client.delete(collection_name=COLLECTION_NAME, filter=f'path == "{abs_path}"')
    except Exception:
        pass

    content_hash = compute_file_hash(path)
    chunks = chunking.chunk_file(path)
    if not chunks:
        return 0

    docs = [c['content'] for c in chunks]
    metas = [{'path': abs_path, 'language': c['language'], 'type': c['type']} for c in chunks]
    ids = [f"{abs_path}::{i}" for i in range(len(chunks))]

    add_documents(docs, metas, ids, content_hash=content_hash, db_path=db_path)
    return len(chunks)


def _get_indexed_paths_under(dir_path: str, db_path: Optional[str] = None) -> set:
    """Get all unique file paths in the index that start with dir_path."""
    try:
        with milvus_client(db_path) as client:
            if not client.has_collection(COLLECTION_NAME):
                return set()
            paths = set()
            escaped = dir_path.replace('"', '\\"')
            iterator = client.query_iterator(
                collection_name=COLLECTION_NAME,
                filter=f'path like "{escaped}%"',
                output_fields=["path"],
                batch_size=1000
            )
            while True:
                batch = iterator.next()
                if not batch:
                    break
                for r in batch:
                    paths.add(r['path'])
            iterator.close()
            return paths
    except Exception:
        return set()


# Default excluded directories
_DEFAULT_EXCLUDED_DIRS = {'node_modules', 'build', 'dist', 'target', 'bin',
                          'test', 'tests', 'ext', 'bower_components',
                          '.sencha', 'locale', 'packages', 'sass',
                          'lib', 'libs', 'vendor', 'vendors',
                          'data', 'venv', 'cdk.out', 'generated'}


def _load_ragignore(project_root: Optional[str] = None) -> Optional[set]:
    """Load .ragignore from the project directory. Returns None if file doesn't exist."""
    if project_root:
        ragignore_path = Path(project_root) / '.ragignore'
    else:
        ragignore_path = Path(__file__).parent / '.ragignore'
    if not ragignore_path.exists():
        return None

    excluded = set()
    with open(ragignore_path, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                excluded.add(line)
    return excluded


def get_excluded_dirs(extra_excludes: Optional[List[str]] = None, project_root: Optional[str] = None) -> set:
    """Get the set of excluded directory names.
    Looks for .ragignore in project_root first, falls back to defaults."""
    ragignore = _load_ragignore(project_root)
    if ragignore is not None:
        excluded = ragignore
    else:
        excluded = set(_DEFAULT_EXCLUDED_DIRS)

    if extra_excludes:
        excluded.update(extra_excludes)

    return excluded


def index_directory(dir_path: str, extensions: Optional[List[str]] = None,
                    incremental: bool = True, progress_callback=None,
                    max_files: int = 0, extra_excludes: Optional[List[str]] = None,
                    jaxb_filter: bool = True, db_path: Optional[str] = None) -> Dict:
    """Index all files in a directory."""
    dir_path = Path(dir_path)
    extensions = extensions or ['.java', '.js', '.ts', '.tsx', '.jsx', '.json', '.xml', '.yaml', '.yml', '.md',
                                '.gradle', '.properties']

    # Derive project root from db_path ({root}/.code-rag/milvus.db) for .ragignore lookup
    project_root = str(Path(db_path).parent.parent) if db_path else None
    excluded_dirs = get_excluded_dirs(extra_excludes, project_root=project_root)

    files_indexed = 0
    files_skipped = 0
    chunks_created = 0
    errors = 0
    by_language = {}
    by_type = {}

    # Collect files
    all_files = []
    for file_path in dir_path.rglob('*'):
        if not file_path.is_file():
            continue
        if file_path.suffix not in extensions:
            continue
        if any(part.startswith('.') for part in file_path.relative_to(dir_path).parts[:-1]):
            continue
        if excluded_dirs & set(file_path.parts):
            continue
        if file_path.suffix in ('.js', '.jsx'):
            if file_path.with_suffix('.ts').exists() or file_path.with_suffix('.tsx').exists():
                continue
        if file_path.name.endswith('.d.ts'):
            continue
        if file_path.stat().st_size > 1024 * 1024:
            continue
        if file_path.name.startswith('.'):
            continue
        if jaxb_filter and file_path.suffix == '.java' and is_jaxb_generated(str(file_path)):
            continue
        all_files.append(file_path)

    if max_files > 0:
        all_files = all_files[:max_files]
    total_files = len(all_files)
    files_removed = 0

    disk_paths = {str(f.absolute()) for f in all_files}

    with milvus_session(db_path):
        if max_files == 0:
            indexed_paths = _get_indexed_paths_under(str(dir_path.absolute()), db_path)
            stale_paths = indexed_paths - disk_paths
            for stale_path in stale_paths:
                delete_by_path(stale_path, db_path)
                files_removed += 1
            if stale_paths:
                print(f"  Removed {len(stale_paths)} stale file(s) from index")

        for idx, file_path in enumerate(all_files):
            try:
                num_chunks = add_file(str(file_path), force=not incremental, db_path=db_path)
                if num_chunks > 0:
                    files_indexed += 1
                    chunks_created += num_chunks

                    lang = chunking.detect_language(str(file_path))
                    t = chunking.detect_type(str(file_path), lang)
                    by_language[lang] = by_language.get(lang, 0) + num_chunks
                    by_type[t] = by_type.get(t, 0) + num_chunks

                    if files_indexed % 10 == 0:
                        mx.clear_cache()
                else:
                    files_skipped += 1

            except Exception as e:
                print(f"Error indexing {file_path}: {e}")
                errors += 1

            if progress_callback:
                progress_callback(idx + 1, total_files, file_path.name)

    return {
        'files_indexed': files_indexed,
        'files_skipped': files_skipped,
        'files_removed': files_removed,
        'chunks_created': chunks_created,
        'errors': errors,
        'by_language': by_language,
        'by_type': by_type
    }


# --- Async wrappers (for HTTP server use) ---

async def search_async(query: str, n: int = 5, type_filter: Optional[str] = None,
                       language_filter: Optional[str] = None, db_path: Optional[str] = None) -> List[Dict]:
    """Async search. Serializes embedding via semaphore, reads are lock-free."""
    loop = asyncio.get_event_loop()

    if _embed_semaphore is not None:
        async with _embed_semaphore:
            query_embedding = await loop.run_in_executor(None, lambda: embed_texts([query])[0])
    else:
        query_embedding = await loop.run_in_executor(None, lambda: embed_texts([query])[0])

    filters = []
    if type_filter:
        filters.append(f'type == "{type_filter}"')
    if language_filter:
        filters.append(f'language == "{language_filter}"')
    filter_expr = " && ".join(filters) if filters else None

    def _do_search():
        with milvus_client(db_path) as client:
            return client.search(
                collection_name=COLLECTION_NAME,
                data=[query_embedding],
                limit=n,
                filter=filter_expr,
                output_fields=["document", "path", "language", "type", "doc_id"]
            )

    results = await loop.run_in_executor(None, _do_search)

    formatted = []
    if results and len(results) > 0:
        for hit in results[0]:
            formatted.append({
                'content': hit['entity']['document'],
                'path': hit['entity'].get('path', ''),
                'language': hit['entity'].get('language', ''),
                'type': hit['entity'].get('type', ''),
                'distance': hit['distance']
            })
    return formatted


async def add_file_async(path: str, force: bool = False, db_path: Optional[str] = None) -> int:
    """Async add_file for use by file watcher. Uses embed semaphore + write lock."""
    loop = asyncio.get_event_loop()
    abs_path = str(Path(path).absolute())

    # Hash check is cheap — do it without locks
    if not force:
        needs = await loop.run_in_executor(None,
            lambda: file_needs_indexing(abs_path, db_path))
        if not needs:
            return 0

    # Embedding + DB write needs both semaphore and lock
    if _embed_semaphore is not None and _write_lock is not None:
        async with _embed_semaphore:
            async with _write_lock:
                return await loop.run_in_executor(None,
                    lambda: add_file(path, force=force, db_path=db_path))
    else:
        return await loop.run_in_executor(None,
            lambda: add_file(path, force=force, db_path=db_path))


async def delete_by_path_async(file_path: str, db_path: Optional[str] = None) -> int:
    """Async delete_by_path for use by file watcher. Uses write lock."""
    loop = asyncio.get_event_loop()

    if _write_lock is not None:
        async with _write_lock:
            return await loop.run_in_executor(None,
                lambda: delete_by_path(file_path, db_path))
    else:
        return await loop.run_in_executor(None,
            lambda: delete_by_path(file_path, db_path))
